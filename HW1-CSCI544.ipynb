{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CSCI544 Assignment 1: Sentiment Analysis on Amazon Reviews Assignment. \n",
    "\n",
    "@author: Ryan Luu \n",
    "\n",
    "@date: 9/8/2022\n",
    "\n",
    "\n",
    "The purpose of this assignment is to experiment with text representations and how to use text representations \n",
    "for sentiment analysis. It takes an dataset of amazon reviews (~ 16 million reviews) and trys to train some \n",
    "classifiers to predict a products rating (1-5 stars) based on the reviews written. \n",
    "\n",
    "E.g) 4-5 star reviews will typically have words like: Great! Good! Happy! \n",
    "\n",
    "     1-2 Star reviews will have words like: Bad! Yuck! Gross! \n",
    "\n",
    "The code flow is like so: \n",
    "  -> Reading in file into pandas\n",
    "  -> preprocessing on dataset (removing html,punctuation, numbers, etc)\n",
    "  -> extracting the tfidf features \n",
    "  -> Training Preceptron, SVM, Logisitc Regression, and Multinomial Naive Bayes classifiers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tokenize import word_tokenize, RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import LinearSVC\n",
    "import contractions\n",
    "\n",
    "\n",
    "import ssl\n",
    "#nltk.download('wordnet')\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bs4 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (0.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from bs4) (4.11.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from beautifulsoup4->bs4) (2.3.2.post1)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 22.2.2 is available.\n",
      "You should consider upgrading via the '/Library/Frameworks/Python.framework/Versions/3.9/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip3 install bs4 # in case you don't have it installed\n",
    "\n",
    "# Dataset: https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Jewelry_v1_00.tsv.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "\n",
    "These are helper functions that I implemented to help with preprocessing the dataset and printing out the average \n",
    "length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Function to convert treebag POS to wordnet POS. No need for ADJ_SAT since we're gooing from POS to wordnet\n",
    "def word_net_pos_converter(pos_tag):\n",
    "    if pos_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif pos_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif pos_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    elif pos_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    else:\n",
    "        return wordnet.NOUN     # word_net defaults to Noun otherwise\n",
    "\n",
    "# Helper function to print out average length of review before and after processing\n",
    "def get_avg_review_len(col, step, flag, df):\n",
    "    mean_len = df[col].apply(len).mean()\n",
    "    if flag == False:\n",
    "        print(\"This is the average length of reviews before \" + step + \" \" + str(mean_len))\n",
    "    else:\n",
    "        print(\"This is the average length of reviews after \" + step + \" \" + str(mean_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_table('amazon_reviews_us_Jewelry_v1_00.tsv', usecols=['star_rating', 'review_body'], low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keep Reviews and Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna()                                  # Gets rid of NaN's in Table\n",
    "data = data.reset_index(drop=True)                    # Resets the index\n",
    "data['star_rating'] = data['star_rating'].astype(int) # Cast ratings to int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## We select 20000 reviews randomly from each rating class.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adds 20000 reviews with 1-5 star reviews. Total is balanced 100,000 reviews.\n",
    "data = data.sample(frac=1).reset_index(drop=True)\n",
    "sampled_amazon_df = data[data['star_rating'] == 1][:20000]\n",
    "sampled_amazon_df = sampled_amazon_df.append(data[data['star_rating']== 2] [:20000])\n",
    "sampled_amazon_df = sampled_amazon_df.append(data[data['star_rating'] == 3][:20000])\n",
    "sampled_amazon_df = sampled_amazon_df.append(data[data['star_rating'] == 4][:20000])\n",
    "sampled_amazon_df = sampled_amazon_df.append(data[data['star_rating'] == 5][:20000])\n",
    "sampled_amazon_df = sampled_amazon_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing\n",
    "\n",
    "For Pre-Processing we have to clean the text before we extract its features. \n",
    "The finished preprocessed text body is stored in our df under review_body column.\n",
    "\n",
    "The cleaning steps taken in this notebook were as follows: \n",
    "\n",
    "- Lowercasing all the words: \"i'm a cat\" -> \"i'm am a cat\"\n",
    "- Fix Contractions: \"i'm a cat\" -> \"i am a cat\"\n",
    "- Remove HTML, HTML Tags, and URL's: \"go check out my soundcloud www.hoTtrash.html\" -> \"go check out my soundcloud\"\n",
    "- Remove all non-alphabetical characters: \" very123 nice\" -> \"very nice\"\n",
    "- Remove extra spaces: \"wowwww         wow: -> \"wowww wow\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the average length of reviews before data cleaning: 189.62446\n",
      "This is the average length of reviews after data cleaning: 183.56328\n"
     ]
    }
   ],
   "source": [
    "get_avg_review_len('review_body', 'data cleaning:', False, sampled_amazon_df)\n",
    "\n",
    "# Lower Case\n",
    "sampled_amazon_df['review_body'] = sampled_amazon_df['review_body'].str.lower()\n",
    "\n",
    "# Fixes Contractions\n",
    "sampled_amazon_df['fix_contractions'] = sampled_amazon_df['review_body'].apply(lambda l: [contractions.fix(word) for word in l.split()])\n",
    "sampled_amazon_df['review_body'] = [' '.join(map(str, k)) for k in sampled_amazon_df['fix_contractions']]\n",
    "del sampled_amazon_df['fix_contractions']\n",
    "\n",
    "# Removes HTML, HTML Tags, and URL's.\n",
    "sampled_amazon_df[\"review_body\"] = sampled_amazon_df[\"review_body\"].str.replace('<[^<]+?>', '', regex=True).str.strip()\n",
    "sampled_amazon_df['review_body'] = sampled_amazon_df['review_body'].replace(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', regex=True)\n",
    "\n",
    "# Removes all non-alphabetical chars\n",
    "sampled_amazon_df['review_body'] = sampled_amazon_df['review_body'].str.replace('[^a-zA-Z]', ' ', regex=True)\n",
    "\n",
    "# removes extra spaces\n",
    "sampled_amazon_df['review_body'] = sampled_amazon_df['review_body'].replace(r'\\s+', ' ', regex=True)\n",
    "\n",
    "get_avg_review_len('review_body', 'data cleaning:', True, sampled_amazon_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## remove the stop words \n",
    "\n",
    "Removes all the stop words in our body using NLTK's list of stopwords and then regex to make the tokens.\n",
    "\n",
    "- Removing Stopwords: \"I am a cat\" -> \"cat\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the average length of reviews before removing stop words and lemmization: 183.56328\n",
      "This is the average length of reviews after removing stop words and lemmization: 16.74663\n"
     ]
    }
   ],
   "source": [
    "# Prints out average length before removing stop words and lemmization\n",
    "get_avg_review_len('review_body', 'removing stop words and lemmization:', False, sampled_amazon_df) \n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "regexp = RegexpTokenizer('\\w+')\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "# Creates column of NLTK tokens\n",
    "sampled_amazon_df[\"nltk_tokens\"] = sampled_amazon_df[\"review_body\"].apply(regexp.tokenize)\n",
    "\n",
    "# Removes the stop words\n",
    "sampled_amazon_df['nltk_tokens'] = sampled_amazon_df['nltk_tokens'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "\n",
    "# Prints out average length after removing stop words and lemmization\n",
    "get_avg_review_len('nltk_tokens', 'removing stop words and lemmization:', True, sampled_amazon_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## perform lemmatization  \n",
    "\n",
    "We lemmatize all the words after we remove the stop words\n",
    "\n",
    "- Lemmatizing words: \"am, are, is\" -> be "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets Parts of Speech for each word\n",
    "sampled_amazon_df['part_of_speech_tags'] = sampled_amazon_df['nltk_tokens'].apply(nltk.tag.pos_tag)\n",
    "\n",
    "# Creates column of Wordnet POS tokens\n",
    "sampled_amazon_df['wordnet_part-of_speech_tags'] = sampled_amazon_df['part_of_speech_tags'].apply(lambda x: [(word,word_net_pos_converter(pos_tag)) for (word,pos_tag) in x])\n",
    "\n",
    "# Lemmatizes the words with NLTK wordnetlemmatizer\n",
    "sampled_amazon_df['lemmatized_reviews'] = sampled_amazon_df['wordnet_part-of_speech_tags'].apply(lambda x: \" \".join([wnl.lemmatize(word,pos_tag) for word,pos_tag in x]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF Feature Extraction\n",
    "\n",
    "TF-IDF is the term frequency–inverse document frequency; a numerical statistic that is intended to reflect how important a word is to a document. \n",
    "\n",
    "We extract this using sk-learn's TfidfVectorizer's. We then then feed in a 80% training and 20% testing set from our dataset of 100,000 reviews (80,000 random reviews from 1-5 stars are for training, 20,000 random reviews from 1-5 stars are for testing). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_vectorizer = TfidfVectorizer()\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(sampled_amazon_df['lemmatized_reviews'], sampled_amazon_df['star_rating'], test_size = 0.20, random_state = 727)\n",
    "#print(\"Train: \", X_train.shape, Y_train.shape,\"Test: \", (X_test.shape, Y_test.shape))\n",
    "tf_x_train = tf_idf_vectorizer.fit_transform(X_train)\n",
    "tf_x_test = tf_idf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score      support\n",
      "1              0.538892  0.650802  0.589584   3992.00000\n",
      "2              0.379596  0.309750  0.341134   4000.00000\n",
      "3              0.393167  0.337391  0.363150   4025.00000\n",
      "4              0.447660  0.406734  0.426217   4069.00000\n",
      "5              0.592989  0.721768  0.651072   3914.00000\n",
      "accuracy       0.483750  0.483750  0.483750      0.48375\n",
      "macro avg      0.470461  0.485289  0.474231  20000.00000\n",
      "weighted avg   0.469731  0.483750  0.473120  20000.00000\n"
     ]
    }
   ],
   "source": [
    "p_classifier = Perceptron(tol=1e-3)\n",
    "p_classifier.fit(tf_x_train, Y_train)\n",
    "y_test_pred = p_classifier.predict(tf_x_test)\n",
    "report = classification_report(Y_test, y_test_pred, output_dict=True)\n",
    "p_df = pd.DataFrame(report).transpose()\n",
    "print(svm_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score      support\n",
      "1              0.538892  0.650802  0.589584   3992.00000\n",
      "2              0.379596  0.309750  0.341134   4000.00000\n",
      "3              0.393167  0.337391  0.363150   4025.00000\n",
      "4              0.447660  0.406734  0.426217   4069.00000\n",
      "5              0.592989  0.721768  0.651072   3914.00000\n",
      "accuracy       0.483750  0.483750  0.483750      0.48375\n",
      "macro avg      0.470461  0.485289  0.474231  20000.00000\n",
      "weighted avg   0.469731  0.483750  0.473120  20000.00000\n"
     ]
    }
   ],
   "source": [
    "svm_classifier = LinearSVC()\n",
    "svm_classifier.fit(tf_x_train, Y_train)\n",
    "y_test_pred = svm_classifier.predict(tf_x_test)\n",
    "report = classification_report(Y_test, y_test_pred, output_dict=True)\n",
    "svm_df = pd.DataFrame(report).transpose()\n",
    "print(svm_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score     support\n",
      "1              0.574497  0.643287  0.606949   3992.0000\n",
      "2              0.403001  0.369250  0.385388   4000.0000\n",
      "3              0.415688  0.387081  0.400875   4025.0000\n",
      "4              0.474213  0.440649  0.456815   4069.0000\n",
      "5              0.634686  0.703117  0.667152   3914.0000\n",
      "accuracy       0.507400  0.507400  0.507400      0.5074\n",
      "macro avg      0.500417  0.508677  0.503436  20000.0000\n",
      "weighted avg   0.499614  0.507400  0.502401  20000.0000\n"
     ]
    }
   ],
   "source": [
    "lr_classifier = LogisticRegression(max_iter=1000, solver='saga')\n",
    "lr_classifier.fit(tf_x_train, Y_train)\n",
    "y_test_pred = lr_classifier.predict(tf_x_test)\n",
    "report = classification_report(Y_test, y_test_pred, output_dict=True)\n",
    "lr_df = pd.DataFrame(report).transpose()\n",
    "print(lr_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score      support\n",
      "1              0.578140  0.604208  0.590887   3992.00000\n",
      "2              0.394772  0.370000  0.381985   4000.00000\n",
      "3              0.398097  0.395031  0.396558   4025.00000\n",
      "4              0.445312  0.420251  0.432419   4069.00000\n",
      "5              0.624499  0.677312  0.649835   3914.00000\n",
      "accuracy       0.492150  0.492150  0.492150      0.49215\n",
      "macro avg      0.488164  0.493360  0.490337  20000.00000\n",
      "weighted avg   0.487282  0.492150  0.489294  20000.00000\n"
     ]
    }
   ],
   "source": [
    "mnb_classifier = MultinomialNB()\n",
    "mnb_classifier.fit(tf_x_train, Y_train)\n",
    "y_test_pred = mnb_classifier.predict(tf_x_test)\n",
    "\n",
    "report = classification_report(Y_test, y_test_pred, output_dict=True)\n",
    "mnb_df = pd.DataFrame(report).transpose()\n",
    "print(mnb_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
